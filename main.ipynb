{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "41ac8810",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch.nn as nn\n",
    "\n",
    "%run dataset.ipynb\n",
    "%run model.ipynb\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "95678ebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#对数据进行填充以保证每个 batch 中的样本具有相同的长度\n",
    "def pad_collate(batch):\n",
    "    (texts, features, emotions) = zip(*batch)\n",
    "    text_lengths = [len(t) for t in texts]\n",
    "    text_padded = torch.zeros(len(texts), max(text_lengths), dtype=torch.long)\n",
    "    for i, t in enumerate(texts):\n",
    "        text_padded[i, :len(t)] = t\n",
    "    return text_padded, torch.stack(features), torch.stack(emotions)\n",
    "\n",
    "def train(model, dataloader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    for batch in dataloader:\n",
    "        text, feature_label, emotion_label = batch\n",
    "        text = text.to(device)\n",
    "        feature_label = feature_label.to(device)\n",
    "        emotion_label = emotion_label.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        emotion_output, feature_output = model(text, feature_label)\n",
    "        emotion_loss = criterion(emotion_output, emotion_label)\n",
    "        feature_loss = criterion(feature_output, feature_label)\n",
    "        loss = emotion_loss + feature_loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "    return epoch_loss / len(dataloader)\n",
    "\n",
    "def evaluate(model, dataloader, criterion, device):\n",
    "    model.eval()\n",
    "    epoch_loss = 0\n",
    "    emotion_correct = 0\n",
    "    feature_correct = 0\n",
    "    total = 0\n",
    "    all_emotion_preds = []\n",
    "    all_feature_preds = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            text, feature_label, emotion_label = batch\n",
    "            text = text.to(device)\n",
    "            feature_label = feature_label.to(device)\n",
    "            emotion_label = emotion_label.to(device)\n",
    "            emotion_output, feature_output = model(text, feature_label)\n",
    "            emotion_loss = criterion(emotion_output, emotion_label)\n",
    "            feature_loss = criterion(feature_output, feature_label)\n",
    "            loss = emotion_loss + feature_loss\n",
    "            epoch_loss += loss.item()\n",
    "            emotion_correct += (emotion_output.argmax(1) == emotion_label).sum().item()\n",
    "            feature_correct += (feature_output.argmax(1) == feature_label).sum().item()\n",
    "            total += emotion_label.size(0)\n",
    "\n",
    "            emotion_preds = torch.argmax(emotion_output, dim=1)\n",
    "            feature_preds = torch.argmax(feature_output, dim=1)\n",
    "\n",
    "            all_emotion_preds.extend(emotion_preds.tolist())\n",
    "            all_feature_preds.extend(feature_preds.tolist())\n",
    "\n",
    "    return epoch_loss / len(dataloader), emotion_correct / total, feature_correct / total, all_emotion_preds, all_feature_preds\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fb472f61",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\pengzy\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 0.741 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "IN: TextClassificationModel_MultiScaleAttention\n",
      "Test Loss: 3.2577, Train Loss: 4.1644, Test Emotion Accuracy: 0.6650, Test Feature Accuracy: 0.1341\n",
      "Test Loss: 1.5834, Train Loss: 2.5343, Test Emotion Accuracy: 0.7615, Test Feature Accuracy: 0.7693\n",
      "Test Loss: 1.1949, Train Loss: 1.4030, Test Emotion Accuracy: 0.7984, Test Feature Accuracy: 0.8637\n",
      "Test Loss: 1.0204, Train Loss: 1.0845, Test Emotion Accuracy: 0.8006, Test Feature Accuracy: 0.9226\n",
      "Test Loss: 0.8752, Train Loss: 0.8953, Test Emotion Accuracy: 0.8048, Test Feature Accuracy: 0.9475\n",
      "Test Loss: 0.8185, Train Loss: 0.7845, Test Emotion Accuracy: 0.8098, Test Feature Accuracy: 0.9652\n",
      "Test Loss: 0.7961, Train Loss: 0.7068, Test Emotion Accuracy: 0.8048, Test Feature Accuracy: 0.9773\n",
      "Test Loss: 0.7688, Train Loss: 0.6943, Test Emotion Accuracy: 0.8048, Test Feature Accuracy: 0.9780\n",
      "Test Loss: 0.7421, Train Loss: 0.6341, Test Emotion Accuracy: 0.8027, Test Feature Accuracy: 0.9815\n",
      "Test Loss: 0.7307, Train Loss: 0.5953, Test Emotion Accuracy: 0.8055, Test Feature Accuracy: 0.9858\n",
      "Test Loss: 0.7081, Train Loss: 0.5749, Test Emotion Accuracy: 0.8062, Test Feature Accuracy: 0.9886\n",
      "Test Loss: 0.7172, Train Loss: 0.5562, Test Emotion Accuracy: 0.8084, Test Feature Accuracy: 0.9908\n",
      "Test Loss: 0.7038, Train Loss: 0.5422, Test Emotion Accuracy: 0.8077, Test Feature Accuracy: 0.9915\n",
      "Test Loss: 0.7192, Train Loss: 0.5365, Test Emotion Accuracy: 0.8055, Test Feature Accuracy: 0.9915\n",
      "Test Loss: 0.7162, Train Loss: 0.5202, Test Emotion Accuracy: 0.7821, Test Feature Accuracy: 0.9929\n",
      "Test Loss: 0.7237, Train Loss: 0.5197, Test Emotion Accuracy: 0.8119, Test Feature Accuracy: 0.9929\n",
      "Test Loss: 0.7345, Train Loss: 0.5095, Test Emotion Accuracy: 0.8091, Test Feature Accuracy: 0.9929\n",
      "Test Loss: 0.7160, Train Loss: 0.5031, Test Emotion Accuracy: 0.8070, Test Feature Accuracy: 0.9929\n",
      "Test Loss: 0.7062, Train Loss: 0.4947, Test Emotion Accuracy: 0.8048, Test Feature Accuracy: 0.9943\n",
      "Test Loss: 0.7338, Train Loss: 0.4869, Test Emotion Accuracy: 0.8048, Test Feature Accuracy: 0.9943\n",
      "Test Loss: 0.7267, Train Loss: 0.4875, Test Emotion Accuracy: 0.8048, Test Feature Accuracy: 0.9936\n",
      "Test Loss: 0.7032, Train Loss: 0.4829, Test Emotion Accuracy: 0.8055, Test Feature Accuracy: 0.9936\n",
      "Test Loss: 0.7060, Train Loss: 0.4819, Test Emotion Accuracy: 0.8098, Test Feature Accuracy: 0.9943\n",
      "Test Loss: 0.7153, Train Loss: 0.4773, Test Emotion Accuracy: 0.8070, Test Feature Accuracy: 0.9936\n",
      "Test Loss: 0.7303, Train Loss: 0.4652, Test Emotion Accuracy: 0.8055, Test Feature Accuracy: 0.9943\n",
      "Test Loss: 0.6859, Train Loss: 0.4764, Test Emotion Accuracy: 0.8077, Test Feature Accuracy: 0.9943\n",
      "Test Loss: 0.7201, Train Loss: 0.4722, Test Emotion Accuracy: 0.8070, Test Feature Accuracy: 0.9943\n",
      "Test Loss: 0.6876, Train Loss: 0.4716, Test Emotion Accuracy: 0.7906, Test Feature Accuracy: 0.9943\n",
      "Test Loss: 0.6911, Train Loss: 0.4686, Test Emotion Accuracy: 0.8055, Test Feature Accuracy: 0.9943\n",
      "Test Loss: 0.6846, Train Loss: 0.4634, Test Emotion Accuracy: 0.8112, Test Feature Accuracy: 0.9943\n",
      "Test Loss: 0.6797, Train Loss: 0.4620, Test Emotion Accuracy: 0.8105, Test Feature Accuracy: 0.9943\n",
      "Test Loss: 0.6766, Train Loss: 0.4667, Test Emotion Accuracy: 0.8041, Test Feature Accuracy: 0.9943\n",
      "Test Loss: 0.6874, Train Loss: 0.4631, Test Emotion Accuracy: 0.8062, Test Feature Accuracy: 0.9943\n",
      "Test Loss: 0.6749, Train Loss: 0.4604, Test Emotion Accuracy: 0.8062, Test Feature Accuracy: 0.9943\n",
      "Test Loss: 0.7002, Train Loss: 0.4576, Test Emotion Accuracy: 0.8041, Test Feature Accuracy: 0.9950\n",
      "Test Loss: 0.6903, Train Loss: 0.4491, Test Emotion Accuracy: 0.8048, Test Feature Accuracy: 0.9957\n",
      "Test Loss: 0.6836, Train Loss: 0.4568, Test Emotion Accuracy: 0.8091, Test Feature Accuracy: 0.9943\n",
      "Test Loss: 0.7137, Train Loss: 0.4571, Test Emotion Accuracy: 0.8084, Test Feature Accuracy: 0.9965\n",
      "Test Loss: 0.7154, Train Loss: 0.4558, Test Emotion Accuracy: 0.8077, Test Feature Accuracy: 0.9957\n",
      "Test Loss: 0.6839, Train Loss: 0.4623, Test Emotion Accuracy: 0.8105, Test Feature Accuracy: 0.9965\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 60\u001b[0m\n\u001b[0;32m     54\u001b[0m             \u001b[38;5;28mprint\u001b[39m(\n\u001b[0;32m     55\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTest Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtest_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Train Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Test Emotion Accuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtest_emotion_accuracy\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Test Feature Accuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtest_feature_accuracy\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     59\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m---> 60\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[3], line 51\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     48\u001b[0m best_test_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mfloat\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minf\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     50\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[1;32m---> 51\u001b[0m     train_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     52\u001b[0m     test_loss, test_emotion_accuracy, test_feature_accuracy, test_emotion_preds, test_feature_preds \u001b[38;5;241m=\u001b[39m evaluate(\n\u001b[0;32m     53\u001b[0m         model, test_dataloader, criterion, device)\n\u001b[0;32m     54\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\n\u001b[0;32m     55\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTest Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtest_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Train Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Test Emotion Accuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtest_emotion_accuracy\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Test Feature Accuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtest_feature_accuracy\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[1;32mIn[2], line 13\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(model, dataloader, optimizer, criterion, device)\u001b[0m\n\u001b[0;32m     11\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m     12\u001b[0m epoch_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m---> 13\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m dataloader:\n\u001b[0;32m     14\u001b[0m     text, feature_label, emotion_label \u001b[38;5;241m=\u001b[39m batch\n\u001b[0;32m     15\u001b[0m     text \u001b[38;5;241m=\u001b[39m text\u001b[38;5;241m.\u001b[39mto(device)\n",
      "File \u001b[1;32m~\\.conda\\envs\\py310\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:634\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    631\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    632\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    633\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 634\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    635\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    636\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    637\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    638\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32m~\\.conda\\envs\\py310\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:678\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    676\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    677\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 678\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    679\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    680\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32m~\\.conda\\envs\\py310\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32m~\\.conda\\envs\\py310\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_19188\\2910783410.py:17\u001b[0m, in \u001b[0;36mTextDataset.__getitem__\u001b[1;34m(self, index)\u001b[0m\n\u001b[0;32m     15\u001b[0m text_tensor \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvocab[token] \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m tokens], dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong)\n\u001b[0;32m     16\u001b[0m feature_tensor \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeature_vocab[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeatures[index]], dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong)\n\u001b[1;32m---> 17\u001b[0m emotion_tensor \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43memotions\u001b[49m\u001b[43m[\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlong\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m text_tensor, feature_tensor, emotion_tensor\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    datasetPath = 'datasets/'\n",
    "    data_file =  datasetPath +'processed_data.csv'\n",
    "    embedding_file = datasetPath + 'sgns.weibo.bigram-char.bz2'\n",
    "    texts, features, emotions = read_csv(data_file)\n",
    "    #分词，构建词汇表\n",
    "    tokenizer = get_tokenizer()\n",
    "    tokenized_texts = [tokenizer(text) for text in texts]\n",
    "    vocab = build_vocab_from_iterator(tokenized_texts)\n",
    "    feature_vocab = build_feature_vocab(features)\n",
    "\n",
    "    train_texts, test_texts, train_features, test_features, train_emotions, test_emotions = train_test_split(texts, features, emotions, test_size=0.2, random_state=42)\n",
    "\n",
    "    train_dataset = TextDataset(train_texts, train_features, train_emotions, tokenizer, vocab,feature_vocab)\n",
    "    test_dataset = TextDataset(test_texts, test_features, test_emotions, tokenizer, vocab,feature_vocab)\n",
    "\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=64, shuffle=True, collate_fn=pad_collate)\n",
    "    test_dataloader = DataLoader(test_dataset, batch_size=64, shuffle=False, collate_fn=pad_collate)\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    num_gpus = torch.cuda.device_count()\n",
    "\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    embeddings = load_embeddings(embedding_file, vocab)\n",
    "    modelList = list()\n",
    "\n",
    "    \n",
    "    modelList.append(TextClassificationModel_MultiScaleAttention(len(vocab), len(feature_vocab), 300, 128, 8, 2, len(feature_vocab), embeddings=embeddings))\n",
    "    \n",
    "    for model in  modelList:\n",
    "        name = model.name\n",
    "        print(\"IN:\",model.name)\n",
    "\n",
    "        if torch.cuda.device_count() > 1:\n",
    "            print(\"Using\", torch.cuda.device_count(), \"GPUs\")\n",
    "            model = nn.DataParallel(model)\n",
    "        model.to(device)\n",
    "        if num_gpus > 1:\n",
    "            model = nn.DataParallel(model)\n",
    "        else:\n",
    "            model = model.to(device)\n",
    "\n",
    "        optimizer = optim.Adam(model.parameters(), lr=0.0002)\n",
    "        criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "        num_epochs = 100\n",
    "        best_test_loss = float('inf')\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "            train_loss = train(model, train_dataloader, optimizer, criterion, device)\n",
    "            test_loss, test_emotion_accuracy, test_feature_accuracy, test_emotion_preds, test_feature_preds = evaluate(\n",
    "                model, test_dataloader, criterion, device)\n",
    "            print(\n",
    "                f'Test Loss: {test_loss:.4f}, Train Loss: {train_loss:.4f}, Test Emotion Accuracy: {test_emotion_accuracy:.4f}, Test Feature Accuracy: {test_feature_accuracy:.4f}')\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
